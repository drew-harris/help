{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T00:06:33.331986Z",
     "start_time": "2023-10-30T00:06:32.989931Z"
    }
   },
   "id": "97c16df46281e13b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "# Drop the TractId column as it is not needed\n",
    "df.drop(['TractId'], axis=1, inplace=True)\n",
    "df.drop(['County'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T00:06:33.356533Z",
     "start_time": "2023-10-30T00:06:33.333322Z"
    }
   },
   "id": "44a18712c3631780"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   State  TotalPop   Men  Women  Hispanic  White  Black  Native  Asian  \\\n",
      "0      0      1192   673    745        24    863     52       0     12   \n",
      "1      0      1519   941    804        11    416    545       0     10   \n",
      "2      0      2732  1307   1651        80    614    265       6      7   \n",
      "3      0      3614  1775   2065        96    803     71       5      2   \n",
      "4      0      8670  4689   4624         9    775    164       0     31   \n",
      "5      0      2967  1539   1654        30    707    251       0      0   \n",
      "6      0      2767  1233   1760        40    780    137       6      0   \n",
      "7      0      2260  1269   1217        42    863     71      15      0   \n",
      "8      0      9101  4930   5154        14    818    153       0     10   \n",
      "9      0      5514  2885   2855        10    862     97       8      3   \n",
      "\n",
      "   Pacific  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  Employed  \\\n",
      "0        0  ...     5            0          21          189       781   \n",
      "1        0  ...     0            5           0          166       752   \n",
      "2        4  ...    10            8          15          175      1382   \n",
      "3        0  ...    15           29          21          203      1749   \n",
      "4        0  ...     8            3           7          154      4628   \n",
      "5        0  ...     7           35          80          155      1264   \n",
      "6        0  ...     0            0           0          108      1318   \n",
      "7        0  ...     0            7          52          190      1269   \n",
      "8        0  ...     0            0          22          218      4690   \n",
      "9        0  ...     0           50          59          300      2681   \n",
      "\n",
      "   PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \n",
      "0          424         211            45           0            46  \n",
      "1          441         149            90           0            34  \n",
      "2          415         210            48           7            47  \n",
      "3          440         196            45           0            61  \n",
      "4          396         240            45           0            23  \n",
      "5          522         140            19           0            61  \n",
      "6          456         175            50           0           169  \n",
      "7          409         174            88          10            72  \n",
      "8          383         224            74           0            22  \n",
      "9          486         144            50           0            68  \n",
      "\n",
      "[10 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# Encode strings to integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = df.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "print(df.head(10))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T00:06:34.025719Z",
     "start_time": "2023-10-30T00:06:33.356471Z"
    }
   },
   "id": "b18b11218c83da95"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   State  TotalPop   Men  Women  Hispanic  White  Black  Native  Asian  \\\n",
      "0      0      1192   673    745        24    863     52       0     12   \n",
      "1      0      1519   941    804        11    416    545       0     10   \n",
      "2      0      2732  1307   1651        80    614    265       6      7   \n",
      "3      0      3614  1775   2065        96    803     71       5      2   \n",
      "4      0      8670  4689   4624         9    775    164       0     31   \n",
      "5      0      2967  1539   1654        30    707    251       0      0   \n",
      "6      0      2767  1233   1760        40    780    137       6      0   \n",
      "7      0      2260  1269   1217        42    863     71      15      0   \n",
      "8      0      9101  4930   5154        14    818    153       0     10   \n",
      "9      0      5514  2885   2855        10    862     97       8      3   \n",
      "\n",
      "   Pacific  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  Employed  \\\n",
      "0        0  ...     5            0          21          189       781   \n",
      "1        0  ...     0            5           0          166       752   \n",
      "2        4  ...    10            8          15          175      1382   \n",
      "3        0  ...    15           29          21          203      1749   \n",
      "4        0  ...     8            3           7          154      4628   \n",
      "5        0  ...     7           35          80          155      1264   \n",
      "6        0  ...     0            0           0          108      1318   \n",
      "7        0  ...     0            7          52          190      1269   \n",
      "8        0  ...     0            0          22          218      4690   \n",
      "9        0  ...     0           50          59          300      2681   \n",
      "\n",
      "   PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \n",
      "0          424         211            45           0            46  \n",
      "1          441         149            90           0            34  \n",
      "2          415         210            48           7            47  \n",
      "3          440         196            45           0            61  \n",
      "4          396         240            45           0            23  \n",
      "5          522         140            19           0            61  \n",
      "6          456         175            50           0           169  \n",
      "7          409         174            88          10            72  \n",
      "8          383         224            74           0            22  \n",
      "9          486         144            50           0            68  \n",
      "\n",
      "[10 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# Divide the rows into 4 groups, with equal number of rows in each group based on the values in the column 'ChildPoverty'. And encode the child poverty value to either be 0, 1, 2, or 3.\n",
    "df['ChildPoverty'] = pd.qcut(df['ChildPoverty'], 4, labels=False)\n",
    "print(df.head(10))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T00:06:34.026141Z",
     "start_time": "2023-10-30T00:06:34.010839Z"
    }
   },
   "id": "6bd5cea07335d8d3"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18229\n",
      "18171\n",
      "18148\n",
      "18170\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(df.loc[df['ChildPoverty'] == 0]))\n",
    "print(len(df.loc[df['ChildPoverty'] == 1]))\n",
    "print(len(df.loc[df['ChildPoverty'] == 2]))\n",
    "print(len(df.loc[df['ChildPoverty'] == 3]))\n",
    "print(len(df.loc[df['ChildPoverty'] == 4]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T00:06:34.141514Z",
     "start_time": "2023-10-30T00:06:34.032983Z"
    }
   },
   "id": "e684a7f67c6d45b8"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Split the data into features and labels\n",
    "X = df.drop(['ChildPoverty'], axis=1)\n",
    "y = df['ChildPoverty']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_frame, X_test_frame, y_train_frame, y_test_frame = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Convert the dataframes to numpy arrays\n",
    "X_train = X_train_frame.values\n",
    "X_test = X_test_frame.values\n",
    "y_train = y_train_frame.values\n",
    "y_test = y_test_frame.values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T00:06:34.194291Z",
     "start_time": "2023-10-30T00:06:34.066553Z"
    }
   },
   "id": "4db5ac2eb24b53bf"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58174, 34)\n",
      "(14544, 34)\n",
      "(58174,)\n",
      "(14544,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T00:06:34.195376Z",
     "start_time": "2023-10-30T00:06:34.170333Z"
    }
   },
   "id": "c6b2ff6856418d5e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# One hot encode the labels\n",
    "def encode_and_bind(original_dataframe, feature_to_encode):\n",
    "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
    "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
    "    res = res.drop([feature_to_encode], axis=1)\n",
    "    return(res)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T00:06:34.195902Z",
     "start_time": "2023-10-30T00:06:34.181373Z"
    }
   },
   "id": "d607c750ea409f23"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "encode_and_bind(X, 'State')\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T00:06:34.259226Z",
     "start_time": "2023-10-30T00:06:34.190794Z"
    }
   },
   "id": "86348654725246bb"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58174, 34)\n",
      "(14544, 34)\n",
      "(58174,)\n",
      "(14544,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_frame, X_test_frame, y_train_frame, y_test_frame = \\\n",
    "    train_test_split(X_norm, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Convert the dataframes to numpy arrays\n",
    "X_train = X_train_frame\n",
    "X_test = X_test_frame\n",
    "y_train = y_train_frame\n",
    "y_test = y_test_frame\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T00:06:34.290230Z",
     "start_time": "2023-10-30T00:06:34.263954Z"
    }
   },
   "id": "edb280b726d2d119"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.3017512   0.37756428  0.06435835  0.63407181  0.63753587 -1.85573584\n",
      "  2.29114991 -0.22661465 -0.50920366 -0.17967672 -0.3544377  -1.41844605\n",
      " -1.38862083 -1.26923601 -1.03349081  1.33578704 -1.44000958  2.35588601\n",
      " -0.28888303  0.70354925  0.06325437  0.36225803 -0.18113726 -0.06609842\n",
      " -0.45028719  0.09023218 -0.43457759  0.0170304   0.31240149  0.41449322\n",
      " -0.24500979 -0.38757705 -0.38837095  1.53613322]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T00:06:34.345875Z",
     "start_time": "2023-10-30T00:06:34.289699Z"
    }
   },
   "id": "8733fdad006a53ac"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "\n",
    "\n",
    "class NeuralNetMLP(object):\n",
    "    \"\"\" Feedforward neural network / Multi-layer perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of output units, should be equal to the\n",
    "        number of unique class labels.\n",
    "    n_features : int\n",
    "        Number of features (dimensions) in the target dataset.\n",
    "        Should be equal to the number of columns in the X array.\n",
    "    n_hidden : int (default: 30)\n",
    "        Number of hidden units.\n",
    "    l1 : float (default: 0.0)\n",
    "        Lambda value for L1-regularization.\n",
    "        No regularization if l1=0.0 (default)\n",
    "    l2 : float (default: 0.0)\n",
    "        Lambda value for L2-regularization.\n",
    "        No regularization if l2=0.0 (default)\n",
    "    epochs : int (default: 500)\n",
    "        Number of passes over the training set.\n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "    alpha : float (default: 0.0)\n",
    "        Momentum constant. Factor multiplied with the\n",
    "        gradient of the previous epoch t-1 to improve\n",
    "        learning speed\n",
    "        w(t) := w(t) - (grad(t) + alpha*grad(t-1))\n",
    "    decrease_const : float (default: 0.0)\n",
    "        Decrease constant. Shrinks the learning rate\n",
    "        after each epoch via eta / (1 + epoch*decrease_const)\n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "    minibatches : int (default: 1)\n",
    "        Divides training data into k minibatches for efficiency.\n",
    "        Normal gradient descent learning if k=1 (default).\n",
    "    random_state : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    cost_ : list\n",
    "      Sum of squared errors after each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l1=0.0, l2=0.0, epochs=500, eta=0.001,\n",
    "                 alpha=0.0, decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=None):\n",
    "\n",
    "        np.random.seed(random_state)\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.w1, self.w2 = self._initialize_weights()\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "\n",
    "    def _encode_labels(self, y, k):\n",
    "        \"\"\"Encode labels into one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : array, shape = [n_samples]\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot : array, shape = (n_labels, n_samples)\n",
    "\n",
    "        \"\"\"\n",
    "        onehot = np.zeros((k, y.shape[0]))\n",
    "        for idx, val in enumerate(y):\n",
    "            onehot[val, idx] = 1.0\n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        w1 = np.random.uniform(-1.0, 1.0,\n",
    "                               size=self.n_hidden*(self.n_features + 1))\n",
    "        w1 = w1.reshape(self.n_hidden, self.n_features + 1)\n",
    "        w2 = np.random.uniform(-1.0, 1.0,\n",
    "                               size=self.n_output*(self.n_hidden + 1))\n",
    "        w2 = w2.reshape(self.n_output, self.n_hidden + 1)\n",
    "        return w1, w2\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute logistic function (sigmoid)\n",
    "\n",
    "        Uses scipy.special.expit to avoid overflow\n",
    "        error for very small input values z.\n",
    "\n",
    "        \"\"\"\n",
    "        # return 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "\n",
    "    def _sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the logistic function\"\"\"\n",
    "        sg = self._sigmoid(z)\n",
    "        return sg * (1.0 - sg)\n",
    "\n",
    "    def _add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "            X_new[:, 1:] = X\n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0] + 1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "        else:\n",
    "            raise AttributeError('`how` must be `column` or `row`')\n",
    "        return X_new\n",
    "\n",
    "    def _feedforward(self, X, w1, w2):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        z3 : array, shape = [n_output_units, n_samples]\n",
    "            Net input of output layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        a1 = self._add_bias_unit(X, how='column')\n",
    "        z2 = w1.dot(a1.T)\n",
    "        a2 = self._sigmoid(z2)\n",
    "        a2 = self._add_bias_unit(a2, how='row')\n",
    "        z3 = w2.dot(a2)\n",
    "        a3 = self._sigmoid(z3)\n",
    "        return a1, z2, a2, z3, a3\n",
    "\n",
    "    def _L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) +\n",
    "                                np.sum(w2[:, 1:] ** 2))\n",
    "\n",
    "    def _L1_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L1-regularization cost\"\"\"\n",
    "        return (lambda_/2.0) * (np.abs(w1[:, 1:]).sum() +\n",
    "                                np.abs(w2[:, 1:]).sum())\n",
    "\n",
    "    def _get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        output : array, shape = [n_output_units, n_samples]\n",
    "            Activation of the output layer (feedforward)\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float\n",
    "            Regularized cost.\n",
    "\n",
    "        \"\"\"\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        L1_term = self._L1_reg(self.l1, w1, w2)\n",
    "        L2_term = self._L2_reg(self.l2, w1, w2)\n",
    "        cost = cost + L1_term + L2_term\n",
    "        return cost\n",
    "\n",
    "    def _get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, shape = [n_hidden_units, n_features]\n",
    "            Gradient of the weight matrix w1.\n",
    "        grad2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Gradient of the weight matrix w2.\n",
    "\n",
    "        \"\"\"\n",
    "        # backpropagation\n",
    "        sigma3 = a3 - y_enc\n",
    "        z2 = self._add_bias_unit(z2, how='row')\n",
    "        sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2)\n",
    "        sigma2 = sigma2[1:, :]\n",
    "        grad1 = sigma2.dot(a1)\n",
    "        grad2 = sigma3.dot(a2.T)\n",
    "\n",
    "        # regularize\n",
    "        grad1[:, 1:] += self.l2 * w1[:, 1:]\n",
    "        grad1[:, 1:] += self.l1 * np.sign(w1[:, 1:])\n",
    "        grad2[:, 1:] += self.l2 * w2[:, 1:]\n",
    "        grad2[:, 1:] += self.l1 * np.sign(w2[:, 1:])\n",
    "\n",
    "        return grad1, grad2\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(X.shape) != 2:\n",
    "            raise AttributeError('X must be a [n_samples, n_features] array.\\n'\n",
    "                                 'Use X[:,None] for 1-feature classification,'\n",
    "                                 '\\nor X[[i]] for 1-sample classification')\n",
    "\n",
    "        a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2)\n",
    "        y_pred = np.argmax(z3, axis=0)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        y : array, shape = [n_samples]\n",
    "            Target class labels.\n",
    "        print_progress : bool (default: False)\n",
    "            Prints progress as the number of epochs\n",
    "            to stderr.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        self.cost_ = []\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y, self.n_output)\n",
    "\n",
    "        delta_w1_prev = np.zeros(self.w1.shape)\n",
    "        delta_w2_prev = np.zeros(self.w2.shape)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_enc = X_data[idx], y_enc[:, idx]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                a1, z2, a2, z3, a3 = self._feedforward(X_data[idx],\n",
    "                                                       self.w1,\n",
    "                                                       self.w2)\n",
    "                cost = self._get_cost(y_enc=y_enc[:, idx],\n",
    "                                      output=a3,\n",
    "                                      w1=self.w1,\n",
    "                                      w2=self.w2)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(a1=a1, a2=a2,\n",
    "                                                  a3=a3, z2=z2,\n",
    "                                                  y_enc=y_enc[:, idx],\n",
    "                                                  w1=self.w1,\n",
    "                                                  w2=self.w2)\n",
    "\n",
    "                delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2\n",
    "                self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))\n",
    "                self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))\n",
    "                delta_w1_prev, delta_w2_prev = delta_w1, delta_w2\n",
    "\n",
    "        return self\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T00:06:34.449326Z",
     "start_time": "2023-10-30T00:06:34.297555Z"
    }
   },
   "id": "a7838bd4cf36066c"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "nn = NeuralNetMLP(n_output=10, \n",
    "                  n_features=X_train.shape[1], \n",
    "                  n_hidden=50, \n",
    "                  l2=0.1, \n",
    "                  l1=0.0, \n",
    "                  epochs=1000, \n",
    "                  eta=0.001,\n",
    "                  alpha=0.001,\n",
    "                  decrease_const=0.00001,\n",
    "                  minibatches=50, \n",
    "                  shuffle=True,\n",
    "                  random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T00:06:47.313898Z",
     "start_time": "2023-10-30T00:06:47.297977Z"
    }
   },
   "id": "7c14d11dc29cfe17"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1000/1000"
     ]
    },
    {
     "data": {
      "text/plain": "<__main__.NeuralNetMLP at 0x12aec8a50>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, print_progress=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T00:11:21.403068Z",
     "start_time": "2023-10-30T00:06:51.999199Z"
    }
   },
   "id": "e45a622d102eb19b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e1d325ad76893c30"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
